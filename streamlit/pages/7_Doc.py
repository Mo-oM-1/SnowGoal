"""
SnowGoal - Documentation Page
"""

import streamlit as st
from pathlib import Path

st.set_page_config(page_title="Doc | SnowGoal", page_icon="ğŸ“–", layout="wide")

st.title("ğŸ“– Documentation")

# Architecture Diagram
st.header("ğŸ“Š Data Pipeline Flow")

st.markdown("""
```
football-data.org API + The Odds API
        |
        â†“ (Snowpark Python Procedures)
RAW Layer (VARIANT JSON)
        |
        â†“ (Streams CDC)
STAGING Views (reads from Streams + LATERAL FLATTEN)
        |
        â†“ (MERGE incremental via Tasks)
SILVER Tables (Clean Data)
        |
        â†“ (INSERT OVERWRITE via Tasks)
GOLD Tables (Aggregations)
        |
        â†“
Streamlit Dashboard
```
""")

st.divider()

# Data Model (ERD)
st.header("ğŸ—„ï¸ Data Model (ERD)")

st.markdown("""
SILVER Layer schema showing all tables and their relationships:
""")

# Display ERD Image
erd_path = Path(__file__).parent.parent / "assets" / "dbml_snowgoal.png"
st.image(str(erd_path), width=900)

st.divider()

# Data Layers
st.header("ğŸ“‚ Data Layers")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("### ğŸŸ¤ RAW Layer (Bronze)")
    st.markdown("""
    **Format:** VARIANT (JSON)

    **Tables:**
    - `RAW_MATCHES`
    - `RAW_STANDINGS`
    - `RAW_SCORERS`
    - `RAW_TEAMS`
    - `RAW_COMPETITIONS`
    - `RAW_ODDS` ğŸ²

    **CDC:** Streams capture changes
    """)

with col2:
    st.markdown("### âšª SILVER Layer")
    st.markdown("""
    **Format:** Structured tables

    **Tables:**
    - `MATCHES`
    - `STANDINGS`
    - `SCORERS`
    - `TEAMS`
    - `COMPETITIONS`
    - `ODDS` ğŸ²

    **Updates:** MERGE incremental
    """)

with col3:
    st.markdown("### ğŸŸ¡ GOLD Layer")
    st.markdown("""
    **Format:** Aggregations

    **Tables (9):**
    - `LEAGUE_STANDINGS`
    - `TOP_SCORERS`
    - `TEAM_STATS`
    - `RECENT_MATCHES`
    - `UPCOMING_FIXTURES`
    - `MATCH_PATTERNS` â­
    - `REFEREE_STATS` â­
    - `GEOGRAPHIC_STATS` â­
    - `ODDS_ANALYSIS` ğŸ²

    **Refresh:** INSERT OVERWRITE via Tasks
    """)

st.divider()

# CDC Section
st.header("ğŸ”„ CDC (Change Data Capture)")

st.markdown("""
### What is CDC?

**Change Data Capture** automatically captures modifications (INSERT, UPDATE, DELETE) in RAW tables using Snowflake **Streams**.

### Why use CDC?
- âœ… **Performance**: Process only changes, not all data
- âœ… **Cost reduction**: Less compute = fewer credits
- âœ… **Traceability**: Track modification history
""")

st.subheader("ğŸ“Œ SQL Query Examples")

# Example 1: Check streams exist
with st.expander("Check that streams exist"):
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

SHOW STREAMS IN SCHEMA RAW;

-- Expected result: 4 streams (MATCHES, SCORERS, STANDINGS, TEAMS)
    """, language="sql")

# Example 2: Check if stream has data
with st.expander("Check if a stream contains data"):
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

-- TRUE = There are pending changes
-- FALSE = No changes (empty stream)

SELECT SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_MATCHES') AS HAS_DATA_MATCHES;
SELECT SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_SCORERS') AS HAS_DATA_SCORERS;
SELECT SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_STANDINGS') AS HAS_DATA_STANDINGS;
SELECT SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_TEAMS') AS HAS_DATA_TEAMS;
SELECT SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_ODDS') AS HAS_DATA_ODDS;
    """, language="sql")

# Example 3: See stream content
with st.expander("View stream content"):
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

SELECT
    METADATA$ACTION,       -- Change type (INSERT, DELETE)
    METADATA$ISUPDATE,     -- TRUE if it's an UPDATE
    METADATA$ROW_ID,       -- Unique row ID
    DATA,                  -- JSON data
    LOADED_AT
FROM RAW.STREAM_RAW_MATCHES
LIMIT 10;
    """, language="sql")

# Example 4: Count pending changes
with st.expander("Count pending changes"):
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

-- Number of changes in each stream
SELECT 'MATCHES' AS STREAM, COUNT(*) AS PENDING_CHANGES
FROM RAW.STREAM_RAW_MATCHES
UNION ALL
SELECT 'SCORERS', COUNT(*) FROM RAW.STREAM_RAW_SCORERS
UNION ALL
SELECT 'STANDINGS', COUNT(*) FROM RAW.STREAM_RAW_STANDINGS
UNION ALL
SELECT 'TEAMS', COUNT(*) FROM RAW.STREAM_RAW_TEAMS;
    """, language="sql")

# Example 5: Test complete cycle
with st.expander("Test complete cycle (Before/After MERGE)"):
    st.markdown("**BEFORE MERGE:**")
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

-- Check streams state BEFORE
SELECT
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_MATCHES') AS MATCHES,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_SCORERS') AS SCORERS,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_STANDINGS') AS STANDINGS,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_TEAMS') AS TEAMS;

-- Expected result: TRUE, TRUE, TRUE, TRUE
    """, language="sql")

    st.markdown("**Execute pipeline:**")
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA COMMON;

-- Manually trigger the complete pipeline
EXECUTE TASK COMMON.TASK_FETCH_ALL_LEAGUES;

-- Wait 2-3 minutes...
    """, language="sql")

    st.markdown("**AFTER MERGE:**")
    st.code("""
USE ROLE SNOWGOAL_ROLE;
USE WAREHOUSE SNOWGOAL_WH_XS;
USE DATABASE SNOWGOAL_DB;
USE SCHEMA RAW;

-- Check streams state AFTER
SELECT
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_MATCHES') AS MATCHES,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_SCORERS') AS SCORERS,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_STANDINGS') AS STANDINGS,
    SYSTEM$STREAM_HAS_DATA('RAW.STREAM_RAW_TEAMS') AS TEAMS;

-- Expected result: FALSE, FALSE, FALSE, FALSE (empty streams)
    """, language="sql")

st.divider()

# Task Orchestration
st.header("ğŸ”„ Task Orchestration (DAG)")

st.markdown("""
### Execution Flow

**Total Tasks:** 10
**Refresh Frequency:** 3x daily (7h, 17h, 00h)

The CRON schedule is optimized for European football match times:
- **7h (morning):** Captures overnight results and lineup updates
- **17h (afternoon):** Pre-match data before evening fixtures (most matches start 18h-21h)
- **00h (midnight):** Post-match results and final statistics
""")

# Display Task Graph
task_graph_path = Path(__file__).parent.parent / "assets" / "task_arch_snowgoal.png"
st.image(str(task_graph_path), width=900)

st.divider()

# --- SECTION LOGGING & OBSERVABILITÃ‰ ---
st.header("ğŸ›¡ï¸ Logging & ObservabilitÃ©")

st.markdown("""
### Pipeline Monitoring
Afin de garantir la fiabilitÃ© des donnÃ©es et la transparence des flux, le projet intÃ¨gre un systÃ¨me de monitoring centralisÃ©. Chaque procÃ©dure Snowpark communique son Ã©tat de santÃ© Ã  une table technique.

- **Table de Logs :** `SNOWGOAL_DB.COMMON.PIPELINE_LOGS`
- **Niveaux de Log :**
    - `INFO` : ExÃ©cution rÃ©ussie Ã  100%.
    - `WARNING` : **SuccÃ¨s Partiel**. UtilisÃ© notamment pour la gestion du *Rate Limiting* (ex: 10 ligues sur 11 chargÃ©es si l'API est saturÃ©e).
    - `ERROR` : Ã‰chec critique nÃ©cessitant une intervention.
""")

col1, col2 = st.columns(2)
with col1:
    st.subheader("ğŸ’¡ RÃ©silience")
    st.write("""
    Le pipeline utilise une stratÃ©gie de **Graceful Degradation** : si l'appel Ã  une ligue spÃ©cifique Ã©choue (404 ou 429), le script logue l'erreur, passe Ã  la suivante et termine son exÃ©cution au lieu de tout bloquer.
    """)
with col2:
    st.subheader("ğŸš€ Performance")
    st.write("""
    L'ingestion utilise le **Batch Inserting** (via Pandas et `write_pandas`). Les logs permettent de valider le volume de records insÃ©rÃ©s Ã  chaque run.
    """)

with st.expander("ğŸ” Voir les derniers Ã©vÃ©nements du pipeline (Live)"):
    try:
        # Import local pour utiliser ta fonction de connexion existante
        from connection import run_query
        
        logs_query = """
            SELECT 
                EVENT_TIME, 
                LEVEL, 
                COMPONENT_NAME, 
                MESSAGE,
                STACK_TRACE
            FROM SNOWGOAL_DB.COMMON.PIPELINE_LOGS 
            ORDER BY EVENT_TIME DESC 
            LIMIT 10
        """
        logs_df = run_query(logs_query)
        st.dataframe(
            logs_df,
            column_config={
                "EVENT_TIME": st.column_config.DatetimeColumn("Timestamp"),
                "MESSAGE": st.column_config.TextColumn("RÃ©sumÃ©", width="large"),
                "STACK_TRACE": st.column_config.TextColumn("DÃ©tails Techniques", width="medium"),
            },
            use_container_width=True,
            hide_index=True
        )
    except Exception as e:
        st.error(f"Erreur de lecture des logs : {e}")

st.divider()

# Technical Stack
st.header("âš™ï¸ Technical Stack")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### Data Layer")
    st.markdown("""
    - **Storage** : Snowflake Tables (VARIANT for JSON)
    - **Processing** : Snowpark Python Procedures
    - **CDC** : Streams for change capture
    - **Orchestration** : Tasks with DAG dependencies
    - **Scheduling** : CRON (7h, 17h, 00h)
    """)

with col2:
    st.markdown("### Infrastructure")
    st.markdown("""
    - **Warehouse** : SNOWGOAL_WH_XS (auto-suspend 60s)
    - **APIs** :
      - football-data.org (matches, standings, scorers)
      - The Odds API (betting odds)
    - **Security** : External Access Integration + Secrets
    - **Frontend** : Streamlit Cloud
    - **Version Control** : Git/GitHub
    """)

st.divider()

# --- SECTION DATA QUALITY ---
st.divider()
st.header("âœ… Audit de QualitÃ© des DonnÃ©es (Data Quality)")

st.info("""
**Pourquoi cette section ?** Un pipeline robuste doit s'auto-contrÃ´ler. 
Ici, nous interrogeons une vue Snowflake qui analyse les anomalies potentielles 
sur les scores, les cotes et la cohÃ©rence des classements Ã  chaque rafraÃ®chissement.
""")

try:
    # ExÃ©cution de la requÃªte sur la vue de monitoring
    dq_df = run_query("SELECT * FROM GOLD.DATA_QUALITY_DASHBOARD")
    
    # Calcul du nombre total d'anomalies
    total_anomalies = dq_df['ANOMALY_COUNT'].sum()
    
    if total_anomalies == 0:
        st.success("ğŸ¯ **QualitÃ© Optimale** : Aucune anomalie dÃ©tectÃ©e dans les tables Silver.")
    else:
        # On affiche un warning car le pipeline tourne, mais avec des points d'attention
        st.warning(f"âš ï¸ **{total_anomalies} anomalies dÃ©tectÃ©es** dans les donnÃ©es sources.")
        
        # Le petit plus pour l'entretien : expliquer l'anomalie Leicester/Sheffield
        st.markdown("""
        > **Note technique** : Les anomalies dÃ©tectÃ©es sur les points (Championship) correspondent Ã  des 
        > sanctions administratives rÃ©elles (retraits de points). Le systÃ¨me identifie correctement 
        > l'Ã©cart entre les rÃ©sultats sportifs et le classement officiel.
        """)

    # Affichage du tableau de bord de monitoring
    st.table(dq_df)

except Exception as e:
    st.error(f"Erreur lors de la rÃ©cupÃ©ration des mÃ©triques de qualitÃ© : {e}")

# Refresh Schedule
st.header("â° Refresh Schedule")

st.info("ğŸ• Data refreshes automatically **3 times daily**: **7h**, **17h**, and **00h** (Europe/Paris timezone)")

st.markdown("""
### Pipeline Execution Order

1. **07:00, 17:00, 00:00** - `TASK_FETCH_ALL_LEAGUES`
   - Calls Snowpark procedure `FETCH_ALL_LEAGUES()`
   - Fetches data from football-data.org API
   - Inserts JSON into RAW tables

2. **After step 1** - `TASK_FETCH_ODDS`
   - Calls Snowpark procedure `FETCH_ODDS()`
   - Fetches betting odds from The Odds API
   - Inserts JSON into RAW_ODDS table
   - Covers all 11 competitions

3. **After step 2** - `TASK_MERGE_TO_SILVER`
   - Calls stored procedure `SP_MERGE_TO_SILVER()`
   - MERGE operations for 6 tables:
     - Matches
     - Standings
     - Scorers
     - Teams
     - Competitions
     - Odds ğŸ²
   - Incremental updates based on Streams CDC

4. **After step 3** - **9 parallel GOLD refresh tasks**
   - All execute simultaneously using `INSERT OVERWRITE`
   - Full refresh of aggregated tables:
     - 5 Business Intelligence tables
     - 3 Advanced Analytics tables (using enrichment columns)
     - 1 Betting Analytics table (ODDS_ANALYSIS) ğŸ²
   - No dependencies between them

**Estimated execution time:** 50mn**
**Rate limit 10 calls per minutes
""")

st.divider()

# Betting Odds Integration
st.header("ğŸ² Betting Odds Integration")

st.markdown("""
### Data Source
**The Odds API** provides real-time betting odds from multiple bookmakers for all 11 competitions.

### Coverage
- ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ Premier League (PL)
- ğŸ‡ªğŸ‡¸ La Liga (PD)
- ğŸ‡©ğŸ‡ª Bundesliga (BL1)
- ğŸ‡®ğŸ‡¹ Serie A (SA)
- ğŸ‡«ğŸ‡· Ligue 1 (FL1)
- ğŸ† Champions League (CL)
- ğŸ‡ªğŸ‡º European Championship (EC)
- ğŸ‡µğŸ‡¹ Primeira Liga (PPL)
- ğŸ‡³ğŸ‡± Eredivisie (DED)
- ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ Championship (ELC)
- ğŸ‡§ğŸ‡· BrasileirÃ£o (BSA)

### Data Points
For each upcoming match:
- **Home/Draw/Away odds** from 10+ bookmakers
- **Implied probabilities** (calculated from odds)
- **Bookmaker margins** (overround %)
- **Best odds** per outcome
- **Average odds** across all bookmakers
- **Last update** timestamp

### Analytics Features
The **Betting Intelligence** dashboard page provides:
1. **Upcoming Matches** - Real-time odds and probabilities
2. **Bookmaker Comparison** - Find best odds per outcome
3. **Value Bets Detector** - Compare odds vs historical H2H data
4. **Match Predictor** - Combine team form (90-day PPG) with odds
5. **Odds Summary** - Games covered, bookmakers count, avg margin

### API Rate Limits
- **500 requests/month** on free tier
- **~120 requests/month used** (11 leagues Ã— 3 fetches/day Ã— 30 days Ã· 8)
- Well within limits âœ…
""")

st.divider()

# Cost Optimization
st.header("ğŸ’° Cost Optimization")

st.markdown("""
### Key Optimizations

1. **Streams (CDC) - Major Cost Saver** ğŸ¯
   - **Without Streams**: MERGE would scan all 1,782+ matches on every refresh â†’ high compute cost
   - **With Streams**: Only process changed records (e.g., 10-50 updated matches) â†’ 95%+ compute reduction
   - Streams use Time Travel metadata (no data duplication), MERGE reads only deltas
   - Zero compute cost when idle, only consumes resources during MERGE execution

2. **Warehouse auto-suspend**: 60 seconds
   - Minimizes idle compute costs

3. **Controlled refresh**: 3x per day (7h, 17h, 00h)
   - Aligned with match schedules, no unnecessary processing

4. **Parallel execution**: GOLD tasks run simultaneously
   - Reduces total execution time

5. **XS Warehouse**: Smallest size for this workload
   - Cost-effective for small data volumes

### Cost Efficiency

- **Streams (CDC)** = Incremental processing instead of full scans
- Task-based orchestration with controlled refresh schedule
- XS warehouse with 60s auto-suspend
""")

st.divider()

# Security
st.header("ğŸ” Security")

st.markdown("""
### Access Control

- **Role-Based Access Control (RBAC)**
  - Role: `SNOWGOAL_ROLE`
  - Warehouse: `SNOWGOAL_WH_XS`
  - Database: `SNOWGOAL_DB`

### API Security

- **External Access Integration**
  - Allowed network rule : `football-data.org`
  - Secret : API key stored in Snowflake
  - No credentials in code

### Best Practices

- No hardcoded credentials
- Minimal privilege principle
- Encrypted data at rest (Snowflake default)
- Audit logs via ACCOUNT_USAGE
""")

st.divider()

# Footer
st.caption("ğŸ“– SnowGoal Documentation | Season 2025-2026 | Built 100% on Snowflake Native Features")
